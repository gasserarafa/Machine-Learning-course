{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Gradient Descent for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "from lab_utils_uni import plt_house_x, plt_contour_wgrad, plt_divergence, plt_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([1.0, 2.0])   #features\n",
    "y_train = np.array([300.0, 500.0])   #target value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Computing cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_costFunction(w,b,x,y):\n",
    "    num_of_examples=x.shape[0]\n",
    "    cost_sum=0\n",
    "    for i in range(num_of_examples):\n",
    "        ith_model=w*x_train[0][i]+b\n",
    "        ith_target=y_train[0][i]\n",
    "        diff_squared=(ith_model-ith_target)**2\n",
    "        cost_sum= cost_sum+diff_squared\n",
    "    cost=cost_sum*(1/(2*num_of_examples))    \n",
    "    return cost\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The linear regression model predicts the target with f(x)=wx+b, so to predict w,b, the cost function is used to get the cost for every choice of w,b. \n",
    "- The aim is to get w,b that have the lowest cost, which will be the most efficient prediction of the target.\n",
    "- The gradient descent optimizes the linear regression model providing the most efficient w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Computing the gradient ( the partial derivative of J wrt w,b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x,y,w,b):\n",
    "    \n",
    "    num_of_examples=x.shape([0])\n",
    "    dJ_dw=0\n",
    "    dJ_db=0\n",
    "    for i in range(m):\n",
    "        f_x=w*x+b\n",
    "        dJ_dw_i=(f_x-y[i])*x\n",
    "        dJ_db_i=(f_x-y[i])\n",
    "        dJ_dw+=dJ_dw_i\n",
    "        dJ_db+=dJ_db_i\n",
    "    dJ_dw=dJ_dw/num_of_examples\n",
    "    dJ_db=dJ_db/num_of_examples\n",
    "    return dJ_dw,dJ_db\n",
    "        \n",
    "     \n",
    "     \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
